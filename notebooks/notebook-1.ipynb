{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/19 02:46:35 WARN Utils: Your hostname, abdou-Nitro-AN515-55 resolves to a loopback address: 127.0.1.1; using 10.188.49.204 instead (on interface wlp0s20f3)\n",
      "23/11/19 02:46:35 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/19 02:46:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "#We can create a SparkConf() object and use it to initialize the spark context\n",
    "conf = SparkConf().setAppName(\"Notebook 1\").setMaster(\"local[4]\") #Initialize spark context using 4 local cores as workers\n",
    "sc = SparkContext(conf=conf)    \n",
    "\n",
    "from pyspark.rdd import RDD"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:46:36.802818194Z",
     "start_time": "2023-11-19T01:46:34.099925557Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def pretty_print_plan(rdd):\n",
    "    for x in rdd.toDebugString().decode().split('\\n'):\n",
    "        print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:46:36.808642397Z",
     "start_time": "2023-11-19T01:46:36.806200721Z"
    }
   },
   "id": "4345573994a14182"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Moby Dick text\n",
    "\n",
    "![MobyDick](https://m.media-amazon.com/images/I/91xMMAxMS1L._SL1500_.jpg)\n",
    "\n",
    "And so begins an incredible odyssey that will take ship, sailors and readers to the edge of darkness. Moby Dick\" is an epic tragedy of terrible dramatic power, the endless, desperate quest of the captain of a drunken ship on a voyage of no return."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efe07bf00aa931fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Words Count"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b66e9c16157af32"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.89 ms, sys: 1.03 ms, total: 4.92 ms\n",
      "Wall time: 882 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": "pyspark.rdd.RDD"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "text_file = sc.textFile(\"../data/mobydick.txt\")\n",
    "type(text_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:11.034317128Z",
     "start_time": "2023-11-18T17:35:10.137050443Z"
    }
   },
   "id": "edf7fbaceb43100"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 ms, sys: 305 µs, total: 1.47 ms\n",
      "Wall time: 25.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = text_file.flatMap(lambda line: line.split(\" \"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:11.120691967Z",
     "start_time": "2023-11-18T17:35:11.025871394Z"
    }
   },
   "id": "b056a230a64170e6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 80 µs, sys: 20 µs, total: 100 µs\n",
      "Wall time: 104 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "not_empty = words.filter(lambda x: x!=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:11.121440678Z",
     "start_time": "2023-11-18T17:35:11.059285627Z"
    }
   },
   "id": "d082b343cc0a7cc"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 0 ns, total: 10.5 ms\n",
      "Wall time: 146 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "key_val = not_empty.map(lambda x: (x, 1))\n",
    "counts= key_val.reduceByKey(lambda x1, x2: x1+x2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:11.313262594Z",
     "start_time": "2023-11-18T17:35:11.109556686Z"
    }
   },
   "id": "1ea463618f5ffb9c"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of different words = 19840 number of words = 115314 nb of occurence per word = 5.81\n",
      "CPU times: user 16.6 ms, sys: 985 µs, total: 17.6 ms\n",
      "Wall time: 1.81 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the number of different words\n",
    "diff_words = counts.count()\n",
    "## get the number of words\n",
    "nb_words = counts.map(lambda x: x[1]).reduce(lambda x, y: x+y)\n",
    "\n",
    "print(f\"number of different words = {diff_words} number of words = {nb_words} nb of occurence per word = {round(nb_words / diff_words, 2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:13.038274681Z",
     "start_time": "2023-11-18T17:35:11.257612308Z"
    }
   },
   "id": "f353676b64e9d67d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Most common words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af5ecac24524412"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 ms, sys: 7.36 ms, total: 64.7 ms\n",
      "Wall time: 205 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Pythonic way\n",
    "p_data = counts.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:13.244884621Z",
     "start_time": "2023-11-18T17:35:13.038706644Z"
    }
   },
   "id": "14b6f23de8329a14"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in Moby Dick:\n",
      "\n",
      "the:\t6611\n",
      "of:\t3460\n",
      "and:\t2969\n",
      "a:\t2466\n",
      "to:\t2339\n",
      "CPU times: user 4.33 ms, sys: 0 ns, total: 4.33 ms\n",
      "Wall time: 4.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "p_data.sort(key=lambda x: x[1])\n",
    "print(\"Most common words in Moby Dick:\\n\\n\"+\"\\n\".join(['%s:\\t%d'%c for c in reversed(p_data[-5:])]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:13.258224608Z",
     "start_time": "2023-11-18T17:35:13.256366242Z"
    }
   },
   "id": "95845b5291d3ae00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can do the stuff on the head node using old python methods but... it doesn't scale if we have HUGE Data\n",
    "\n",
    "---\n",
    "\n",
    "We are gonna use PySpark RDDs to add this scalable feature"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7763cf6a143f5d25"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 ms, sys: 3.51 ms, total: 21.3 ms\n",
      "Wall time: 301 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# spark way\n",
    "reversed_counts = counts.map(lambda x: (x[1], x[0]))\n",
    "sorted_counts = reversed_counts.sortByKey(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:14.492487071Z",
     "start_time": "2023-11-18T17:35:14.191637055Z"
    }
   },
   "id": "111d4e1011108c42"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2) PythonRDD[19] at RDD at PythonRDD.scala:53 []\n",
      " |  MapPartitionsRDD[17] at mapPartitions at PythonRDD.scala:160 []\n",
      " |  ShuffledRDD[16] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      " +-(2) PairwiseRDD[15] at sortByKey at <timed exec>:3 []\n",
      "    |  PythonRDD[14] at sortByKey at <timed exec>:3 []\n",
      "    |  MapPartitionsRDD[5] at mapPartitions at PythonRDD.scala:160 []\n",
      "    |  ShuffledRDD[4] at partitionBy at NativeMethodAccessorImpl.java:0 []\n",
      "    +-(2) PairwiseRDD[3] at reduceByKey at <timed exec>:2 []\n",
      "       |  PythonRDD[2] at reduceByKey at <timed exec>:2 []\n",
      "       |  ../data/mobydick.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\n",
      "       |  ../data/mobydick.txt HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []\n"
     ]
    }
   ],
   "source": [
    "## Execution plan of the ops\n",
    "pretty_print_plan(sorted_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:15.281810906Z",
     "start_time": "2023-11-18T17:35:15.270104535Z"
    }
   },
   "id": "65a6c499c9d830"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOst 10 common words in Moby Dick:\n",
      "the\t:\t\t\t6611\n",
      "of\t:\t\t\t3460\n",
      "and\t:\t\t\t2969\n",
      "a\t:\t\t\t2466\n",
      "to\t:\t\t\t2339\n",
      "in\t:\t\t\t1969\n",
      ";\t:\t\t\t1949\n",
      "that\t:\t\t\t1430\n",
      "his\t:\t\t\t1275\n",
      "I\t:\t\t\t1180\n",
      "CPU times: user 6.32 ms, sys: 197 µs, total: 6.52 ms\n",
      "Wall time: 178 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Now we come back to the head node bc we need the data now\n",
    "first_10 = sorted_counts.take(10)\n",
    "print(\"MOst 10 common words in Moby Dick:\\n\"+\"\\n\".join([f\"{k}\\t:\\t\\t\\t{v}\" for (v, k) in first_10]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T17:35:16.849038279Z",
     "start_time": "2023-11-18T17:35:16.669409717Z"
    }
   },
   "id": "a37f67777257abe8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### To sum up\n",
    "\n",
    "- An RDD is a distributed immutable array and is the core data structure of Spark\n",
    "- It is not possible to operate on RDD directly but through **Transformations** and **Actions**\n",
    "- **Transformations** transform an RDD into another RDD\n",
    "- **Actions** output their results on the head node\n",
    "- After the action is done you are now using the head node and no longer on the workers node.\n",
    "- RDD operations (**Transformations** and **Actions**) are added to what we call an **Execution Plan**\n",
    "- The plan is executed when the result is needed\n",
    "- It is possible to store intermediate result explicitly by using caching\n",
    "- For scalability you need to use RDDs instead of working only on the head node (Ex: finding the most common words)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b57bccc91e0a596"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Meteorological data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15cda33599f6ab36"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdou/anaconda3/envs/pyspark-env/lib/python3.8/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "%pylab inline\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:47:06.260314973Z",
     "start_time": "2023-11-19T01:47:06.128459567Z"
    }
   },
   "id": "51851e17982c4f6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/NY.parquet\"\n",
    "\n",
    "df = sqlContext.read.load(data_path)\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:50:01.539355807Z",
     "start_time": "2023-11-19T01:49:59.131308537Z"
    }
   },
   "id": "ddac64c2d75ad82d"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:50:13.483996160Z",
     "start_time": "2023-11-19T01:50:11.422860364Z"
    }
   },
   "id": "ff1a25fc7bf5e3a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### projection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80d683fbdecf1b58"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----+\n",
      "|       dist_coast|              Values|Year|\n",
      "+-----------------+--------------------+----+\n",
      "|361.8320007324219|[00 00 00 00 00 0...|1945|\n",
      "|361.8320007324219|[99 46 52 46 0B 4...|1946|\n",
      "|361.8320007324219|[79 4C 75 4C 8F 4...|1947|\n",
      "|361.8320007324219|[72 48 7A 48 85 4...|1948|\n",
      "|361.8320007324219|[BB 49 BC 49 BD 4...|1949|\n",
      "|361.8320007324219|[6E 4B 93 4B BB 4...|1950|\n",
      "|361.8320007324219|[27 4A 32 4A 28 4...|1951|\n",
      "|361.8320007324219|[54 4B 60 4B 6A 4...|1952|\n",
      "|361.8320007324219|[48 4A 37 4A 28 4...|1953|\n",
      "|361.8320007324219|[DE 4A D4 4A CA 4...|2000|\n",
      "|361.8320007324219|[D9 44 C7 44 B6 4...|2001|\n",
      "|361.8320007324219|[CF 4B B8 4B A1 4...|2002|\n",
      "|361.8320007324219|[18 4B F1 4A D2 4...|2003|\n",
      "|361.8320007324219|[CE 4A 9C 4A 6B 4...|2004|\n",
      "|361.8320007324219|[DD 4C D3 4C C9 4...|2005|\n",
      "|361.8320007324219|[91 4B 9F 4B AC 4...|2006|\n",
      "|361.8320007324219|[39 4E 36 4E 34 4...|2007|\n",
      "|361.8320007324219|[3A 4A 11 4A EA 4...|2008|\n",
      "|361.8320007324219|[5C 4A 3F 4A 26 4...|2009|\n",
      "|361.8320007324219|[A1 48 97 48 8E 4...|2010|\n",
      "+-----------------+--------------------+----+\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dist_coast\", 'Values', 'Year').show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T01:56:51.742635003Z",
     "start_time": "2023-11-19T01:56:51.510319472Z"
    }
   },
   "id": "d6a2000bf3a784e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### filter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac06fa415c3e225"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   TMAX_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   SNOW_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   TMIN_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   SNWD_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|       PRCP|1945|[00 7E 00 7E 00 7...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|       SNOW|1945|[00 7E 00 7E 00 7...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|       SNWD|1945|[00 7E 00 7E 00 7...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|       TMAX|1945|[00 7E 00 7E 00 7...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|       TMIN|1945|[00 7E 00 7E 00 7...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.Year==1945 ) & (df.name==\"DANSVILLE MUNI AP\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:01:13.386640005Z",
     "start_time": "2023-11-19T02:01:12.846874242Z"
    }
   },
   "id": "bafd71427af926a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### filter and projection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ad90a66b28939a7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------------------+-----------------+\n",
      "|    Station|      latitude|         longitude|             name|\n",
      "+-----------+--------------+------------------+-----------------+\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "|USW00094704|42.57080078125|-77.71330261230469|DANSVILLE MUNI AP|\n",
      "+-----------+--------------+------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.Year==1945 ) & (df.name==\"DANSVILLE MUNI AP\")).select(\"Station\", \"latitude\", \"longitude\", \"name\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-19T02:03:28.760038728Z",
     "start_time": "2023-11-19T02:03:28.522002523Z"
    }
   },
   "id": "c3b66c44aaf613d7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c8fab7d0b66c345d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### To sum up\n",
    "\n",
    "- Dataframes are an efficient way to store data tables\n",
    "- All of the values in a column have same type (it's like a table column in a DB)\n",
    "- A good way to store a dataframe in disk is to use a Parquet File format"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84589e5a0873dbd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2bc61db65e1387ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
